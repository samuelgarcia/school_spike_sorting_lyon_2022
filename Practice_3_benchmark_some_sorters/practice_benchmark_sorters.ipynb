{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking spike sorters\n",
    "\n",
    "\n",
    "To benchmark the spike sorting algorithms, one of the easiest option is to design artifical dataset, in order to know exactly what is created, and to have a proper \"ground truth\" to compare with. \n",
    "\n",
    "To do so, MEArec is a python package that can help you to generate such artificial datasets. Basically, given some templates and a probe layout, the software will generate traces that can later be used for benchmarking the sorting algorithms. In this notebook, we'll try to get a quick overview of the features allowed by MEArec such that you can test the limits of various spike sorters. \n",
    "\n",
    "The comparison between different sorters can be tedious, since every one of them has a different file format. However, spikeinterface can act as a universal wrapper allowing you to launch and read the results of the sorters. Moreover, spikeinterface comes with numerous analysis functions that will allow you to quickly assess the quality of a spike sorting, and compute quality metrics with respect to the Ground Truth that have been generated\n",
    "\n",
    "\n",
    "The only file needed here will be \"templates_Neuronexus-32_100.h5\", that we will use to generate the artificial recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/datalad/cmd.py:377: RuntimeWarning: coroutine 'run_async_cmd' was never awaited\n",
      "  new_loop = True\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/home/pierre/github/spikeinterface/spikeinterface/core/datasets.py:12: RuntimeWarning: coroutine 'run_async_cmd' was never awaited\n",
      "  HAVE_DATALAD = False\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import MEArec as mr\n",
    "\n",
    "import neo\n",
    "import quantities as pq\n",
    "\n",
    "import spikeinterface.full as si"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : generation of the recordings\n",
    "\n",
    "Here is a small code that will generate a recording given an already generated \"templates\" file. We want to generate an artificial recording with 10 cells, randomly aranged on a Neuronexus layout. The sampling rate will be 30kHe, and the firing rate of the neurons will be arbitrarily set to 5Hz.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = Path('.')\n",
    "template_filename = basedir / 'templates_Neuronexus-32_100.h5'\n",
    "\n",
    "\n",
    "duration = 30*60 # duration (in s) of the recording\n",
    "n_cell = 10 # number of cells we want to create\n",
    "probe = 'Neuronexus-32' # probe layout\n",
    "recording_filename = basedir / f'recordings_collision_{n_cell}cells_{probe}_{duration:0.0f}s.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some parameters should be provided\n",
    "fs = 30000. # sampling rate (in Hz)\n",
    "spikerate = 5. # firing rate of the cells (in Hz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to generate the spike trains that we want to inject into our traces. The easiest thing is to generate Poisson spike trains. To do so, we will rely on MEARec to generate an artificial recording, given our pre-generated templates and a desired firing rate for the neurons, acting as Poisson sources (default in MEARec). The amount of noise in the recording can be controlled via the parameter noise_level. In order to control exactly the data that will be generated, we will also force the seed of the templates (to always use the same) and of the spiketrains (to always generate the same spike trains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spiketrains seed:  40\n",
      "Noise Level  5\n",
      "Templates selection seed:  8075\n",
      "Selecting cells\n",
      "Padding template edges\n",
      "Elapsed pad time: 0.13539767265319824\n",
      "Elapsed resample time: 0.00801229476928711\n",
      "Creating time jittering\n",
      "Elapsed jitter time: 0.26135802268981934\n",
      "Computing spike train SNR\n",
      "Adding spiketrain annotations\n",
      "Convolution seed:  5764\n",
      "Electrode modulaton\n",
      "Adding noise\n",
      "Noise seed:  7382\n",
      "Elapsed time:  146.46042824399956\n"
     ]
    }
   ],
   "source": [
    "rec_params = mr.get_default_recordings_params()\n",
    "rec_params['recordings']['fs'] = fs\n",
    "rec_params['recordings']['sync_rate'] = None\n",
    "rec_params['recordings']['sync_jitter'] = 5\n",
    "rec_params['recordings']['noise_level'] = 5\n",
    "rec_params['recordings']['filter'] = False\n",
    "rec_params['recordings']['chunk_duration'] = 10.\n",
    "\n",
    "rec_params['spiketrains']['f_exc'] = spikerate\n",
    "rec_params['spiketrains']['seed'] = 42\n",
    "rec_params['spiketrains']['duration'] = duration\n",
    "rec_params['spiketrains']['n_exc'] = n_cell\n",
    "rec_params['spiketrains']['n_inh'] = 0\n",
    "\n",
    "rec_params['templates']['n_overlap_pairs'] = None\n",
    "rec_params['templates']['min_dist'] = 0\n",
    "rec_params['templates']['seed'] = 42\n",
    "\n",
    "recgen = mr.gen_recordings(params=rec_params, \n",
    "            templates=template_filename, verbose=True,\n",
    "            n_jobs=1, tmp_mode='memmap')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we can save our recording\n",
    "mr.save_recording_generator(recgen, filename=recording_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : opening and ploting the signals and the spikes from our ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to use spikeinterface to load/play/visualize the traces that have just been generated. For example, you should use the MEArecRecordingExtractor/MEArecSortingExtractor objects to load the MEArec recording (both the traces and the spike times). The recording should be called rec_gt and the sorting sorting_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Loading the recording and the sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have loaded the data, you can plot the timeseries with the widgets provided by spikeinterface, such as for example plot_timeseries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Plot the time series between 5 and 6 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can also, from the same object, plot the raster plot since all the spikes were included in the MEArecSortingExtractor. To do so, use the plot_rasters() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Plot all the spikes between 5 and 6 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to check that your neurons are indeed behaving as Poisson sources, could you find a widget to plot the auto/crosscorrelograms for your population? What do you see, and what do you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Plot the correlograms for all units, and describe what you should see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : running several sorters on our recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With spikeinterface, launching a spike sorting algorithm is easy. You simply need to do use the run_sorter command. Assuming we want to save every sorting into a specific folder, with a name dedicated to every sorter, we can simply do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING SHELL SCRIPT: spykingcircus/run_spykingcircus.sh\n",
      "h5py version > 2.10.0. Some extractors might not work properly. It is recommended to downgrade to version 2.10.0: \n",
      ">>> pip install h5py==2.10.0\n"
     ]
    }
   ],
   "source": [
    "sorting_spykincircus = si.run_sorter('spykingcircus', rec, output_folder=basedir / 'spykingcircus')\n",
    "sorting_tridesclous = si.run_sorter('tridesclous', rec, output_folder=basedir / 'tridesclous')\n",
    "\n",
    "# Note that the sorter list can includes 'kilosort', 'herdingspikes', ... See spikeinterface wrapper for more info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a given sorter has been launched, you can load its results easily. Can you load the results, using the read_sorter_folder() command, and plot a raster plot similar to the one you plotted for the ground truth units, just before? Do they match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just noticed how easy it is to run several sorters on a given recording. But what about comparisons? Now we would like to be able to quantify how good these sorters are, with respect to the ground truth units that have been created. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : running comparison and ploting agreement matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a given spike sorting, we can always compare it to its ground-truth, via the comparison object offered by spikeinterface. To do so, you can use the function compare_sorter_to_ground_truth() that will create a GroundTruthComparison object that you will call comp, comparing the ground truth data with a sorting you just obtained, with the sorter of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create a comp object to compare a sorter and a ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a comparison object can immediatly tell us how many units are found, what are the errors rates, and much more. For example, try to understand how to visualize the agreement_scores between all the units (ground truth vs find ones). What are these scores? Please have a to the classical ways of measuring how \"good\" a sorter is. What are the mathematical definition of accuracy, precision, recall? What about F1 score? \n",
    "To know more about this comparison module, have a look to the documentation https://spikeinterface.readthedocs.io/en/latest/module_comparison.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Try to see ho to obtain agreement_scores, or quality metrics from the comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using the plot_agreement_matrix() command, you can have a quick look to the accuracy of your spike sorter, by comparing how good ground truth units are \"found\" by the sorter, and if these matches are unique. Again, have a look to the documentation to understand what this agreement matrix really means. What does that means if it is not squared, but rectangular? Larger in the horizontal or the vertical direction? What if the diagonal is not filled with 1, and/or if a line have several high values in it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Plot the agreeement matrix between a sorter and the ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 : using the study object and plotting performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we have several spike sorting algorithms, and want to quickly compare each of them, in order to get a sense of the pros and cons of every sorters? This can also be done easily in spikeinterface via the study object. The study can compare several sorters not only on a single, but on multiple recordings at once. In order to probe its potentiel, let's generate a second recording via MEAreac, for example with a larger noise level, but same spike trains. Note that if you prefer, you could also change the firing rate, by generating new spike trains. Depends what you want to probe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise Level  8\n",
      "Templates selection seed:  4893\n",
      "Selecting cells\n",
      "Padding template edges\n",
      "Elapsed pad time: 0.1422104835510254\n",
      "Elapsed resample time: 0.01348733901977539\n",
      "Creating time jittering\n",
      "Elapsed jitter time: 0.25230884552001953\n",
      "Computing spike train SNR\n",
      "Adding spiketrain annotations\n",
      "Convolution seed:  3538\n",
      "Electrode modulaton\n",
      "Adding noise\n",
      "Noise seed:  5793\n",
      "Elapsed time:  145.80792285200005\n",
      "Deleted /tmp/tmp68mxnjzt\n"
     ]
    }
   ],
   "source": [
    "recording_filename_2 = basedir / f'recordings_collision_{n_cell}cells_{probe}_{duration:0.0f}s_noisy.h5'\n",
    "\n",
    "rec_params['recordings']['noise_level'] = 8\n",
    "\n",
    "recgen = mr.gen_recordings(params=rec_params,  \n",
    "            templates=template_filename, verbose=True,\n",
    "            n_jobs=1, tmp_mode='memmap')\n",
    "\n",
    "# Then we can save our recording\n",
    "mr.save_recording_generator(recgen, filename=recording_filename_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by default, the seed used to select n_cell templates out of the pre-generated dictionary is not the same, thus the sorting will be different. But this could have been controled via MEArec, in order to change only the noise level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you should try to load the new recording/sorting you just created, as you did for the first one. The recording could be named rec_2, and the sorting sorting_gt_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Loading the second recording and the associated spikes (ground truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the GroundTruthStudy object, create a study in a folder of your choice (could simply be 'study') such that you will compare sorters on the two differents recordings you have created. Note that the study object accepts a dict of sortings as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can create the dictionary of all recordings used by the study\n",
    "gt_dict = {'rec0' : (rec, sorting_gt), 'rec1' : (rec_2, sorting_gt_2)}\n",
    "\n",
    "#### Create a study object that will compare the two recordings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The study object is gathering all the recordings in the specified folder, this is why we need to specify chunk_memory and number of jobs used to copy evertyhing. Now that we have the study created, we can easily launch it on several sorters, either for all recordings or for a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load the study with the run_sorters() command, on the sorters of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the study has been ran, we need to perform the comparisons between all sorters and the ground truths. Note that if we have an exhaustive ground truth, i.e. if we have a full description of our artificial recordings with the sorting (which is the case here), then it must be specified. This will enhance the quality metrics and provide more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.run_comparisons(exhaustive_gt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can plot the performances over all recordings, and for all sorters, using the plot_gt_study_performances command. What are the pros and cons of the sorters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Comparing the sorters with the plot_gt_study_performences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Comparing the sorters with the plot_gt_study_performences_averages()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
